# -*- mode: ruby -*-
# vi: set ft=ruby :

# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
vagrantfile_api_version = "2"

Vagrant.configure(vagrantfile_api_version) do |config|
  config.vm.define :admin, :primary => true do |admin|
    admin.vm.box = "suse/cloud3-admin"
    #admin.vm.box_url = "http://UPLOAD/ME/images/suse/cloud3-admin.box"
    admin.vm.box_check_update = false

    admin.vm.provider "virtualbox" do |provider|
      provider.memory = 2048
      provider.cpus = 1

      # Don't use headless mode
      provider.gui = true
    end

    admin.vm.provider "libvirt" do |provider|
      provider.nested = true
      provider.volume_cache = "none"

      # Don't use headless mode
      provider.gui = true
    end

    admin.vm.network :forwarded_port, guest: 3000, host: 3000
    # admin.ssh.forward_agent = true

    admin.vm.synced_folder ".", "/vagrant", disabled: true
    #admin.vm.synced_folder "/mnt/suse-cloud-3", "/srv/tftpboot/repos/Cloud", type: "nfs"

    # Setup network for admin node and for Crowbar in general.
    # Vagrant requires the first interface of every VM to be NAT,
    # which with VirtualBox means it can't communicate with the other
    # VMs.  And for an HA setup we need two interfaces which are
    # teamed and able to communicate with other VMs.  On VirtualBox
    # this means making them host-only.
    admin.vm.network "private_network", ip: "192.168.124.10"
    # Additional NIC for bonding. The IP address here is just to make vagrant
    # happy it's not actually used inside the VM
    admin.vm.network "private_network", ip: "192.168.124.9", auto_config: false

    admin.vm.provision "file", source: "cloud3-admin/network.json", destination: "/tmp/network.json"
    # Normally Crowbar seizes control of *all* interfaces.  But in the Vagrant
    # case we don't want it to touch eth0, so we need this evil patch:
    admin.vm.provision "file", source: "cloud3-admin/barclamp-network.patch", destination: "/tmp/barclamp-network.patch"
    admin.vm.provision "file", source: "cloud3-admin/barclamp-provisioner.patch", destination: "/tmp/barclamp-provisioner.patch"
    admin.vm.provision "shell", inline: <<-EOSCRIPT
        cd /opt/dell/
        patch -p1 < /tmp/barclamp-network.patch
        patch -p1 < /tmp/barclamp-provisioner.patch
        cp /tmp/network.json /etc/crowbar/network.json
        rm -f /tmp/network.json
        rm -f /tmp/barclamp-network.patch
    EOSCRIPT

    # Automatically install SUSE Cloud on first-boot
    admin.vm.provision "shell", inline: <<-EOSCRIPT
       export PATH="$PATH:/sbin:/usr/sbin/"
       export REPOS_SKIP_CHECKS="SLES11_SP3 SLES11-SP3-Pool"
       # To trick install-suse-clouds check for "screen". It should be save
       # to run with screen here. As install-suse-cloud won't pull the network
       # from eth0 because of the above patch.
       export STY="dummy"
       install-suse-cloud -v
    EOSCRIPT
  end

  2.times do |i|
    node_name = "controller#{i+1}"
    config.vm.define node_name do |node|
      node.vm.box = 'suse/sles11-sp3'
      #node.vm.box_url = 'http://UPLOAD/ME/images/suse/sles11-sp3.box'
      node.vm.box_check_update = false

      node.vm.provider 'virtualbox' do |provider|
        provider.memory = 2048
        provider.cpus = 1

        # Don't use headless mode
        provider.gui = true

        # create shared disk for SBD
        provider.customize [ 'createhd', '--filename', 'sbd.vmdk', '--size', 8, '--format', 'VMDK', '--variant', 'Fixed' ]
        provider.customize [ 'modifyhd', 'sbd.vmdk', '--type', 'shareable' ]
        provider.customize [
          'storageattach', :id,
          '--storagectl', 'IDE',
          '--port', 0,
          '--device', 1,
          '--type', 'hdd',
          '--medium', 'sbd.vmdk',
        ]
        # create shared disk for glance
        provider.customize [ 'createhd', '--filename', 'glance.vmdk', '--size', 5000, '--format', 'VMDK', '--variant', 'Fixed' ]
        provider.customize [ 'modifyhd', 'glance.vmdk', '--type', 'shareable' ]
        provider.customize [
          'storageattach', :id,
          '--storagectl', 'IDE',
          '--port', 1,
          '--device', 0,
          '--type', 'hdd',
          '--medium', 'glance.vmdk',
        ]
        # create shared disk for cinder
        provider.customize [ 'createhd', '--filename', 'cinder.vmdk', '--size', 5000, '--format', 'VMDK', '--variant', 'Fixed' ]
        provider.customize [ 'modifyhd', 'cinder.vmdk', '--type', 'shareable' ]
        provider.customize [
          'storageattach', :id,
          '--storagectl', 'IDE',
          '--port', 1,
          '--device', 1,
          '--type', 'hdd',
          '--medium', 'cinder.vmdk',
        ]
        # FIXME: We need to either switch to SATA or SCSI or add another IDE controller 
        # to the VM before we can attache a 5th disk to the machine.
        ## create disk for DRBD
        #provider.customize [ 'createhd', '--filename', 'drbd-#{node_name}.vmdk', '--size', 1100, '--format', 'VMDK' ]
        #provider.customize [
        #  'storageattach', :id,
        #  '--storagectl', 'IDE',
        #  '--port', 2,
        #  '--device', 0,
        #  '--type', 'hdd',
        #  '--medium', 'drbd-#{node_name}.vmdk',
        #]
      end

      node.vm.provider 'libvirt' do |provider|
        provider.nested = true
        provider.volume_cache = 'none'

        # Don't use headless mode
        provider.gui = true
      end

      node.vm.synced_folder ".", "/vagrant", disabled: true

      node.vm.network "private_network", ip: "192.168.124.#{81+(i*2-1)}", auto_config: false
      # Additional NIC for bonding. The IP address here is just to make vagrant
      # happy it's not actually used inside the VM
      node.vm.network "private_network", ip: "192.168.124.#{81+(i*2)}", auto_config: false

      node.vm.provision "shell", inline: <<-EOSCRIPT
        export PATH="$PATH:/sbin:/usr/sbin/"
        zypper ar http://192.168.124.10:8091/suse-11.3/install sles11-sp3
        wget http://192.168.124.10:8091/suse-11.3/crowbar_register
        chmod a+x crowbar_register
        ./crowbar_register --force --interface eth1  --gpg-auto-import-keys
      EOSCRIPT
    end
  end

  1.times do |i|
    node_name = "compute#{i+1}"
    config.vm.define node_name do |node|
      node.vm.box = 'suse/sles11-sp3'
      #node.vm.box_url = 'http://UPLOAD/ME/images/suse/sles11-sp3.box'
      node.vm.box_check_update = false

      node.vm.provider 'virtualbox' do |provider|
        provider.memory = 2048
        provider.cpus = 1

        # Don't use headless mode
        provider.gui = true
      end

      node.vm.provider 'libvirt' do |provider|
        provider.nested = true
        provider.volume_cache = 'none'

        # Don't use headless mode
        provider.gui = true
      end

      node.vm.synced_folder ".", "/vagrant", disabled: true

      node.vm.network "private_network", ip: "192.168.124.#{81+(i*2-1)}", auto_config: false
      # Additional NIC for bonding. The IP address here is just to make vagrant
      # happy it's not actually used inside the VM
      node.vm.network "private_network", ip: "192.168.124.#{81+(i*2)}", auto_config: false

      node.vm.provision "shell", inline: <<-EOSCRIPT
        export PATH="$PATH:/sbin:/usr/sbin/"
        zypper ar http://192.168.124.10:8091/suse-11.3/install sles11-sp3
        wget http://192.168.124.10:8091/suse-11.3/crowbar_register
        chmod a+x crowbar_register
        ./crowbar_register --force --interface eth1  --gpg-auto-import-keys
      EOSCRIPT
    end
  end

  # Not tested yet
  config.vm.provider :libvirt do |libvirt|
    libvirt.host = "localhost"
    libvirt.connect_via_ssh = true

    libvirt.username = "root"
    libvirt.password = "linux"

    libvirt.storage_pool_name = "default"
  end
end
