# -*- mode: ruby -*-
# vi: set ft=ruby :

# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
vagrantfile_api_version = "2"

def provision_to_tmp(node, files_to_provision)
  files_to_provision.each do |source|
    filename = File.basename(source)
    node.vm.provision "file", source: source, destination: "/tmp/#{filename}"
  end
end

Vagrant.configure(vagrantfile_api_version) do |config|
  config.vm.define :admin, :primary => true do |admin|
    admin.vm.box = "suse/cloud3-admin"
    #admin.vm.box_url = "http://UPLOAD/ME/images/suse/cloud3-admin.box"
    admin.vm.box_check_update = false

    admin.vm.provider "virtualbox" do |provider|
      provider.memory = 2048
      provider.cpus = 1

      # Don't use headless mode
      provider.gui = true
    end

    admin.vm.provider "libvirt" do |provider|
      provider.nested = true
      provider.volume_cache = "none"

      # Don't use headless mode
      provider.gui = true
    end

    admin.vm.network :forwarded_port, guest: 3000, host: 3000
    # admin.ssh.forward_agent = true

    admin.vm.synced_folder ".", "/vagrant", disabled: true
    #admin.vm.synced_folder "/mnt/suse-cloud-3", "/srv/tftpboot/repos/Cloud", type: "nfs"

    # Setup network for admin node and for Crowbar in general.
    # Vagrant requires the first interface of every VM to be NAT,
    # which with VirtualBox means it can't communicate with the other
    # VMs.  And for an HA setup we need two interfaces which are
    # teamed and able to communicate with other VMs.  On VirtualBox
    # this means making them host-only.
    admin.vm.network "private_network", ip: "192.168.124.10", auto_config: false
    # Additional NIC for bonding. The IP address here is just to make vagrant
    # happy it's not actually used inside the VM
    admin.vm.network "private_network", ip: "192.168.124.9", auto_config: false

    files_to_provision = [
      "cloud3-admin/network.json",
      # Normally Crowbar seizes control of *all* interfaces.  But in the Vagrant
      # case we don't want it to touch eth0, so we need this evil patch:
      "cloud3-admin/barclamp-network-ignore-eth0.patch",
      "cloud3-admin/barclamp-provisioner-demo-repos.patch",

      # https://github.com/crowbar/barclamp-network/pull/286
      "cloud3-admin/barclamp-network-teaming-mode.patch",

      # https://github.com/crowbar/barclamp-provisioner/pull/303
      "cloud3-admin/crowbar_register-hostname.patch",

      # https://github.com/crowbar/barclamp-pacemaker/pull/101 and
      # https://github.com/crowbar/barclamp-pacemaker/pull/103
      "cloud3-admin/install-xfsprogs.patch",

      # https://github.com/crowbar/barclamp-pacemaker/pull/102
      "cloud3-admin/sbd-wait-for-corosync.patch",

      # https://github.com/crowbar/barclamp-crowbar/pull/1024
      "cloud3-admin/apache-mod_status.patch",

      # https://github.com/crowbar/barclamp-database/pull/84 and
      # https://github.com/crowbar/barclamp-rabbitmq/pull/42
      "cloud3-admin/drbd-race.patch",
    ]
    provision_to_tmp(admin, files_to_provision)
    admin.vm.provision "shell", inline: <<-'EOSCRIPT'
        set -e

        cd /opt/dell/
        patch -p1 < /tmp/barclamp-network-ignore-eth0.patch
        patch -p1 < /tmp/barclamp-network-teaming-mode.patch
        patch -p1 < /tmp/barclamp-provisioner-demo-repos.patch
        patch -p1 < /tmp/crowbar_register-hostname.patch
        patch -p1 < /tmp/install-xfsprogs.patch
        patch -p1 < /tmp/sbd-wait-for-corosync.patch
        patch -p1 < /tmp/apache-mod_status.patch
        patch -p1 < /tmp/drbd-race.patch

        cp /tmp/network.json /etc/crowbar/network.json
        rm -f /tmp/network.json

        # Scrap pointless 45 second tcpdump per interface
        sed -i 's/45/5/' /opt/dell/chef/cookbooks/ohai/files/default/plugins/crowbar.rb

        # Bypass the provisioner's check for HAE repos, since we're already
        # providing these via the special SUSE-CLOUD-3-DEPS installation media
        # in /srv/tftpboot/suse-11.3/install
        for repo in SLE11-HAE-SP3-{Pool,Updates}; do
            ln -s ../suse-11.3/install /srv/tftpboot/repos/$repo
        done
    EOSCRIPT

    # Automatically install SUSE Cloud on first-boot
    admin.vm.provision "shell", inline: <<-EOSCRIPT
       export PATH="$PATH:/sbin:/usr/sbin/"
       export REPOS_SKIP_CHECKS="SLES11_SP3 SLES11-SP3-Pool"
       # To trick install-suse-clouds check for "screen". It should be save
       # to run with screen here. As install-suse-cloud won't pull the network
       # from eth0 because of the above patch.
       export STY="dummy"
       install-suse-cloud -v
    EOSCRIPT
  end

  2.times do |i|
    node_name = "controller#{i+1}"
    config.vm.define node_name do |node|
      node.vm.box = 'suse/sles11-sp3'
      #node.vm.box_url = 'http://UPLOAD/ME/images/suse/sles11-sp3.box'
      node.vm.box_check_update = false

      node.vm.provider 'virtualbox' do |provider|
        provider.memory = 2048
        provider.cpus = 1

        # Don't use headless mode
        provider.gui = true

        ## create disk for DRBD
        provider.customize [ 'createhd', '--filename', "drbd-#{node_name}.vmdk", '--size', 2100, '--format', 'VMDK' ]
        provider.customize [
          'storageattach', :id,
          '--storagectl', 'SCSI',
          '--port', 1,
          '--device', 0,
          '--type', 'hdd',
          '--medium', "drbd-#{node_name}.vmdk",
        ]
        # create shared disk for SBD
        provider.customize [ 'createhd', '--filename', 'sbd.vmdk', '--size', 8, '--format', 'VMDK', '--variant', 'Fixed' ]
        provider.customize [ 'modifyhd', 'sbd.vmdk', '--type', 'shareable' ]
        provider.customize [
          'storageattach', :id,
          '--storagectl', 'SCSI',
          '--port', 2,
          '--device', 0,
          '--type', 'hdd',
          '--medium', 'sbd.vmdk',
        ]
        # create shared disk for glance
        provider.customize [ 'createhd', '--filename', 'glance.vmdk', '--size', 1000, '--format', 'VMDK', '--variant', 'Fixed' ]
        provider.customize [ 'modifyhd', 'glance.vmdk', '--type', 'shareable' ]
        provider.customize [
          'storageattach', :id,
          '--storagectl', 'SCSI',
          '--port', 3,
          '--device', 0,
          '--type', 'hdd',
          '--medium', 'glance.vmdk',
        ]
      end

      node.vm.provider 'libvirt' do |provider|
        provider.nested = true
        provider.volume_cache = 'none'

        # Don't use headless mode
        provider.gui = true
      end

      # Allow Hawk web UI to be accessed by a presentation laptop.
      node.vm.network :forwarded_port, guest: 7630, host: 7630+i-1

      node.vm.synced_folder ".", "/vagrant", disabled: true

      node.vm.network "private_network", ip: "192.168.124.#{81+(i*2-1)}", auto_config: false
      # Additional NIC for bonding. The IP address here is just to make vagrant
      # happy it's not actually used inside the VM
      node.vm.network "private_network", ip: "192.168.124.#{81+(i*2)}", auto_config: false

      files_to_provision = [
        # https://bugzilla.novell.com/show_bug.cgi?id=877484
        "cloud3-admin/crmsh-report-bnc877484.patch",
      ]
      provision_to_tmp(node, files_to_provision)

      node.vm.provision "shell", inline: <<-EOSCRIPT
        set -e

        export PATH="$PATH:/sbin:/usr/sbin/"
        zypper ar http://192.168.124.10:8091/suse-11.3/install sles11-sp3
        until [ -f crowbar_register ]; do
          wget http://192.168.124.10:8091/suse-11.3/crowbar_register
        done
        chmod a+x crowbar_register
        ./crowbar_register --force --interface eth1  --gpg-auto-import-keys

        zypper -n in crmsh # can't patch crmsh until it's installed
        patch -d /usr/lib64/python2.6/site-packages/crmsh -p2 \
            < /tmp/crmsh-report-bnc877484.patch

        # Set up SBD disk
        zypper -n in sbd
        sbd -d /dev/sdc create

        # Format glance disk
        echo y | mkfs.ext3 /dev/sdd
        echo "/dev/sdd /var/lib/glance ext3 defaults 0 2" >> /etc/fstab
        mkdir -p /var/lib/glance
        mount /var/lib/glance
      EOSCRIPT
    end
  end

  1.times do |i|
    node_name = "compute#{i+1}"
    config.vm.define node_name do |node|
      node.vm.box = 'suse/sles11-sp3'
      #node.vm.box_url = 'http://UPLOAD/ME/images/suse/sles11-sp3.box'
      node.vm.box_check_update = false

      node.vm.provider 'virtualbox' do |provider|
        provider.memory = 2048
        provider.cpus = 1

        # Don't use headless mode
        provider.gui = true
      end

      node.vm.provider 'libvirt' do |provider|
        provider.nested = true
        provider.volume_cache = 'none'

        # Don't use headless mode
        provider.gui = true
      end

      node.vm.synced_folder ".", "/vagrant", disabled: true

      node.vm.network "private_network", ip: "192.168.124.#{81+(i*2-1)}", auto_config: false
      # Additional NIC for bonding. The IP address here is just to make vagrant
      # happy it's not actually used inside the VM
      node.vm.network "private_network", ip: "192.168.124.#{81+(i*2)}", auto_config: false

      node.vm.provision "shell", inline: <<-EOSCRIPT
        set -e

        export PATH="$PATH:/sbin:/usr/sbin/"
        zypper ar http://192.168.124.10:8091/suse-11.3/install sles11-sp3
        until [ -f crowbar_register ]; do
          wget http://192.168.124.10:8091/suse-11.3/crowbar_register
        done
        chmod a+x crowbar_register
        ./crowbar_register --force --interface eth1  --gpg-auto-import-keys
      EOSCRIPT
    end
  end

  # Not tested yet
  config.vm.provider :libvirt do |libvirt|
    libvirt.host = "localhost"
    libvirt.connect_via_ssh = true

    libvirt.username = "root"
    libvirt.password = "linux"

    libvirt.storage_pool_name = "default"
  end
end
